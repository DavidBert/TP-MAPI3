{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practical session: Neural networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lab we will implement the different elements that compose a neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the first part, we will try to predict the selling price of real estate in the city Boston.  \n",
    "To do so we will use the classic boston house-prices dataset available directly in the machine learning libraryg [scikit-learn](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_boston.html)  \n",
    "So let's start by importing the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_boston\n",
    "dataset = load_boston()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will therefore try to predict the price of houses from 13 features presented [here](https://scikit-learn.org/stable/datasets/index.html#boston-dataset).   \n",
    "\n",
    "Let's start by separating the dataset in 2:\n",
    "* A learning set to train the model\n",
    "* A test set to evaluate the learned model\n",
    "\n",
    "Use [train_test_split](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html)  from scikit-learn to separate the dataset as follows:\n",
    "* X_train -> the features to train the model\n",
    "* y_train -> prices to predict during learning\n",
    "* X_test -> the features to test the model\n",
    "* y_test -> labels to evaluate the model's predictions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/boston_train_test_split.py\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "data = np.asarray(dataset.data, dtype='float32')\n",
    "target = np.asarray(dataset.target.reshape(-1, 1), dtype='float32')\n",
    "\n",
    "X_train, X_test, y_train, y_test = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Among the best practices for improving the learning of neural networks, it is important to normalize the input data to obtain an average close to 0 and a variance to 1.\n",
    "Use the [StandardScaler](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html) method of scikit-learn to normalize X_train and X_test as well as y_train and y_test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/boston_scaler.py\n",
    "from sklearn import preprocessing\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to build our neural networks as successions of layers..  \n",
    "We will distinguish, similarly to the most common frameworks (Tensorflow, Pytorch, Keras ...) two types of layers:\n",
    "* weight and gradient layers\n",
    "* activation layers \n",
    "\n",
    "Each layer will have:\n",
    "* a forward method which receives as input a vector $x$ and apply a transformation to it that will serve as inputs for the following layers\n",
    "* a backward method which receives as input gradients and backpropagate them to the previous layers \n",
    "* a layer_type argument used to define the type of layer\n",
    "\n",
    "The following class defines the structure of a layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer:\n",
    "    def __init__(self):\n",
    "        self.layer_type = 'abstract'\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        pass\n",
    "    \n",
    "    def backward(self, grad):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A neural network will therefore be a class:\n",
    "* containing a list of layers\n",
    "* having a forward method (which will be in fact the composition of the forward methods of its layers) which for a vector x will provide a prediction\n",
    "* and a backward method (composition of the backward methods of its layers) which will backpropagate the gradients from its last layer to its first layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![NeuralNetwork](layers.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now implement the layers of our network.  \n",
    "Let's start with the \"linear\" layers composed of weight $W$ and bias $b$.  \n",
    "These layers receive a vector $x$ as input and return the result of $Wx + b$.\n",
    "\n",
    "Complete the definition of the LinearLayer class:\n",
    "* weights and biases will be stored in a dictionary named \"params\". The weights will be represented by a matrix $input \\times output$ initialized uniformly between $[-0.1, 0.1]$ (you may use random.uniform function from numpy)\n",
    "* implement the forward method (you can use the np.matmul function to calculate $Wx$),  it will be necessary to keep in memory the input vector which will be used when calling the backward method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%load solutions/layers.py\n",
    "class Linear(Layer):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.layer_type = 'linear'\n",
    "        self.params ={}\n",
    "        self.grads = {}\n",
    "        #weights est la matrice [input x output] contenant les poids de la couche \n",
    "        self.params[\"weights\"] = ...\n",
    "        #biais est un vecteur de dimension: (output_size)\n",
    "        self.params[\"bias\"] = ...\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        # outputs = inputs @ weights + biais\n",
    "        #Nous devons garder en mémoire les inputs car ils seront utilisés dans la méthode backward\n",
    "        self.inputs = inputs\n",
    "        return ...\n",
    "    \n",
    "    def backward(self, grad):\n",
    "        # we need to sum gradients over the batch axis\n",
    "        self.grads[\"weights\"] = np.matmul(self.inputs.T, grad)\n",
    "        self.grads[\"bias\"] = np.sum(grad, axis=0)\n",
    "        return np.matmul(grad, self.params[\"weights\"].T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test your layer, the following cell should not return an error:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size, output_size = 10, 5\n",
    "X = np.random.rand(input_size)\n",
    "grads = np.random.rand(10,5)\n",
    "test_linear_layer = Linear(input_size, output_size)\n",
    "assert test_linear_layer.forward(X).shape == (5,)\n",
    "assert test_linear_layer.backward(grads).shape == (10, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activation Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now implement the activation functions used by neurons.\n",
    "Implement an activation function and its derivative (for example the ReLU function: $f(x) = max(0,x)$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/activations.py\n",
    "def relu(x):\n",
    "    return ...\n",
    "\n",
    "def relu_prime(x):\n",
    "    return ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now implement the activation layers:  \n",
    "They will be initialized with a function $f$ and its derivative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%load  solutions/activation_layer.py\n",
    "class Activation(Layer):\n",
    "    def __init__(self, f, f_prime):\n",
    "        super().__init__()\n",
    "        self.layer_type = 'activation'\n",
    "        self.f = f\n",
    "        self.f_prime = f_prime\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        #Nous devons garder en mémoire les inputs car ils seront utilisés dans la méthode backward\n",
    "        self.intputs = inputs\n",
    "        return ...\n",
    "    \n",
    "    def backward(self, grad):\n",
    "        \"\"\" si y = f(x) et x = g(z)\n",
    "        alors dy/dz = f'(x) * g'(z)\n",
    "        Dans notre cas g'(z) correspond aux gradients en entrée\n",
    "        \"\"\"\n",
    "        return self.f_prime(self.intputs) * grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test your activation layer, the following cell should not return an error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relu_layer = Activation(f=relu, f_prime=relu_prime)\n",
    "\n",
    "#forward\n",
    "x = np.array([-1, 5, -2, 6])\n",
    "y = relu_layer.forward(x)\n",
    "assert np.array_equal(y, np.array([0, 5, 0, 6]))\n",
    "\n",
    "#backward\n",
    "incomming_grads = np.array([-0.1, 0.6, -0.4, -0.1])\n",
    "grads = relu_layer.backward(incomming_grads)\n",
    "assert np.array_equal(grads, np.array([-0. ,  0.6, -0. , -0.1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now define the class that will be used to represent our neural networks.  \n",
    "A neural network will have a list of layers and two methods: forward and backward.  \n",
    "\n",
    "Complete the forward method and the backward method of the NeuralNetwork class by calling the forward and backward methods of the network layers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/neural_network.py\n",
    "class NeuralNetwork:\n",
    "    def __init__(self, layers):\n",
    "        self.layers = layers\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        ...\n",
    "\n",
    "    def backward(self, grad):\n",
    "        ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have all the elements that compose a neural network.  \n",
    "Instantiate a neural network composedof 3 layers:\n",
    "* an input layer receiving a dimension 13 vector and consisting of 40 neurons\n",
    "* a hidden layer of 40 neurons\n",
    "* an output layer consisting of a single neuron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/boston_network.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test your network on the test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "y_pred = net.forward(X_test)\n",
    "print(mean_absolute_error(y_pred, y_test))\n",
    "    \n",
    "plt.figure(figsize=(20,5))\n",
    "plt.scatter(range(len(y_test)), scaler.inverse_transform(y_test), label='target')\n",
    "plt.scatter(range(len(y_test)), scaler.inverse_transform(y_pred), label='prediction')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The network predictions are bad.  \n",
    "This is normal, we haven't trained him yet.  \n",
    "Let's start by implementing the loss function that the neural network will use in its learning.  \n",
    "We are here in a regression problem, we will use the mean squared error to learn our model:\n",
    "\n",
    "$$\\sum_{i}^{n}\\frac{(f(x_i) - y_i)^2}{n}$$  \n",
    "Implement the loss function as well as its derivative which will be used during the back-propagation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/mse.py\n",
    "class MSE():\n",
    "\n",
    "    def loss(self, y_pred, y_true):\n",
    "        #returns a scalar\n",
    "        return ...\n",
    "    \n",
    "    def grad(self, y_pred, y_true):\n",
    "        #returns a tensor of gradients\n",
    "        return ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now implement the optimization method used during learning.  \n",
    "We will use here the stochastic gradient descent.  \n",
    "The method is simple: for each step we will update the network parameters thanks to the following formula: suivante: $w_{t+1} = w_t - \\eta \\nabla$ où $\\eta$ is the learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/sgd.py\n",
    "class SGD():\n",
    "    def __init__(self, lr=0.001):\n",
    "        self.lr = lr\n",
    "    \n",
    "    def step(self, net):\n",
    "        for layer in net.layers:\n",
    "            if layer.layer_type == 'linear':\n",
    "                for param, grad in zip(layer.params.values(), layer.grads.values()):\n",
    "                    ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fit function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "t only remains to implement the fit function, which achieves an epoch of learning.\n",
    "The function must:\n",
    "* compute network predictions on the learning batch\n",
    "* compute the loss\n",
    "* compute the gradient of the loss\n",
    "* re-propagate the gradients\n",
    "* make an optimization step\n",
    "* return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/fit.py\n",
    "def fit(net, loss, optimizer, X, y):\n",
    "    ...\n",
    "    return prediction_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now train our network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = SGD(lr=0.01)\n",
    "loss = MSE()\n",
    "for i, (x, y) in enumerate(zip(X_train, y_train)):\n",
    "    training_loss = fit(net, loss, optimizer, np.array([x]), np.array([y]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the results of your training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = net.forward(X_test)\n",
    "print(mean_absolute_error(y_pred, y_test))\n",
    "    \n",
    "plt.figure(figsize=(20,5))\n",
    "plt.scatter(range(len(y_test)), scaler.inverse_transform(y_test), label='target')\n",
    "plt.scatter(range(len(y_test)), scaler.inverse_transform(y_pred), label='prediction')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Digits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ENow let's try our class on a classification problem.\n",
    "We will seek to classify handwritten figures from pixels. We will use for this the [digits dataset from scikit-learn](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_digits.html)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_digits\n",
    "\n",
    "dataset = load_digits()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display the first 10 elements of the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load  solutions/digits.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to the previous dataset, separate your data set in two and normalize the input vectors. (No need this time to normalize the y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/digits_preprocessing.py\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "data = np.asarray(dataset.data, dtype='float32')\n",
    "target = np.asarray(dataset.target, dtype='int32')\n",
    "\n",
    "X_train, X_test, y_train, y_test = ...\n",
    "\n",
    "...\n",
    "X_train = ...\n",
    "X_test = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This time we are faced with a classification problem.  \n",
    "The MSE loss function is no longer suitable for our problem.  \n",
    "This time we will use the [cross-entropy.](https://en.wikipedia.org/wiki/Cross_entropy#Cross-entropy_loss_function_and_logistic_regression) as a loss function:\n",
    "$$ L = \\frac{1}{N}\\sum_{i=1}^{N}y_i \\cdot log(\\hat{y_i})$$  \n",
    "With $\\hat{y_i}$ the network prediction on which the softmax function is applied:\n",
    "$$softmax_j(x) = \\frac{e^x_j}{\\sum_{k}e^x_k}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    return np.exp(x) / np.exp(x).sum(axis=-1,keepdims=True)\n",
    "\n",
    "class CrossEntropy():\n",
    "\n",
    "    def loss(self, logits, y_true):\n",
    "        logits_for_answers = logits[np.arange(len(logits)), y_true]\n",
    "        xentropy = - logits_for_answers + np.log(np.sum(np.exp(logits),axis=-1))\n",
    "        return xentropy\n",
    "\n",
    "\n",
    "\n",
    "    def grad(self, logits, y_true):\n",
    "        ones_for_answers = np.zeros_like(logits)\n",
    "        ones_for_answers[np.arange(len(logits)), y_true] = 1\n",
    "        y_softmax = softmax(logits)\n",
    "        return (- ones_for_answers + y_softmax) / logits.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instantiate a neural network capable of predicting the class of an image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%load solutions/digits_net.py\n",
    "...\n",
    "net = ...\n",
    "optimizer = SGD(lr=0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the accuracy of your network on the test set. (You can use the function [acuracy_score](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html) from scikit-learn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%load  solutions/accuracy1.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function allows you to view our predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_prediction(net, sample_idx=range(3), classes=range(10)):\n",
    "    \n",
    "    for idx in sample_idx:\n",
    "        plt.figure()\n",
    "        logits = net.forward(X_test[idx])\n",
    "        probas = softmax(logits)\n",
    "        prediction = np.argmax(probas)\n",
    "\n",
    "        fig, (ax0, ax1) = plt.subplots(nrows=1, ncols=2, figsize=(10, 4))\n",
    "\n",
    "        ax0.imshow(scaler.inverse_transform(X_test[idx]).reshape(8, 8), cmap=plt.cm.gray_r,\n",
    "                   interpolation='nearest')\n",
    "        ax0.set_title(\"True image label: %d\" % y_test[idx]);\n",
    "        ax1.bar(classes, np.eye(len(classes))[y_test[idx]], label='true')\n",
    "        ax1.bar(classes, probas, label='prediction', color=\"red\")\n",
    "        ax1.set_xticks(classes)\n",
    "\n",
    "        ax1.set_title('Output probabilities (prediction: %d)'\n",
    "                      % prediction)\n",
    "        ax1.set_xlabel('Digit class')\n",
    "        ax1.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_prediction(net, sample_idx=range(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This time, we will use mini-batches during our learning.\n",
    "The following function iterates over mini-batches:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iterate_minibatches(inputs, targets, batchsize, shuffle=True):\n",
    "    assert inputs.shape[0] == targets.shape[0]\n",
    "    if shuffle:\n",
    "        indices = np.arange(inputs.shape[0])\n",
    "        np.random.shuffle(indices)\n",
    "    for start_idx in range(0, inputs.shape[0] - batchsize + 1, batchsize):\n",
    "        if shuffle:\n",
    "            excerpt = indices[start_idx:start_idx + batchsize]\n",
    "        else:\n",
    "            excerpt = slice(start_idx, start_idx + batchsize)\n",
    "        yield inputs[excerpt], targets[excerpt]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the following function to realize a learning epoch using mini-batches:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_one_epoch(X, y_true):\n",
    "    for x, y in iterate_minibatches(X, y_true, 32):\n",
    "        training_loss = fit(net, loss, optimizer, x, y)\n",
    "        \n",
    "    return training_loss.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train your network over 15 epochs.  \n",
    "At each epoch calculate your accuracy on the train set and on the test set and display them on the same graph once the training is over."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%load solutions/plot_accuracy.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display your accuracy on the test set and visualize your predictions using the plot_prediction function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/accuracy2.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try to display your confusion matrix by taking inspiration from [this code:](https://scikit-learn.org/dev/auto_examples/classification/plot_digits_classification.html#sphx-glr-auto-examples-classification-plot-digits-classification-py) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/confusion_matrix.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display the examples on which the network is mistaken:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/plot_errors.py"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
