{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Travaux pratiques: Reseaux de neurones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans ce TP nous allons implémenter les differents éléments qui composent un reseau de neurones "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans la première partie, nous allons essayer de prédire le prix de vente de biens immobiliers de la ville Boston.  \n",
    "Nous allons pour cela utiliser le classique jeu de données boston house-prices disponible directement dans la librairie de machine learning [scikit-learn](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_boston.html)  \n",
    "Commençons donc par importer le dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_boston\n",
    "dataset = load_boston()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous allons donc essayer de prédire le prix des maisons à partir de 13 features présentées [ici](https://scikit-learn.org/stable/datasets/index.html#boston-dataset).   \n",
    "\n",
    "Commençons par séparer le dataset en 2:\n",
    "* Un ensemble d'apprentissage pour entrainer le modèle\n",
    "* Un ensemble de test pour tester le modèle appris\n",
    "\n",
    "Utilisez la méthode [train_test_split](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html) de scikit-learn pour séparer le dataset de la façon suivante:\n",
    "* X_train -> les features pour entrainer le modèle\n",
    "* y_train -> les prix à prédire durant l'apprentissage\n",
    "* X_test -> les features pour tester le modèle\n",
    "* y_test -> les prix à prédire pour tester le modèle  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/boston_train_test_split.py\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "data = np.asarray(dataset.data, dtype='float32')\n",
    "target = np.asarray(dataset.target.reshape(-1, 1), dtype='float32')\n",
    "\n",
    "X_train, X_test, y_train, y_test = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parmi les bonnes pratiques permettant d'améliorer l'apprentissage des réseaux de neurones, il est important de normaliser les données en entrée pour obtenir un moyenne proche de 0 et une variance à 1.  \n",
    "Utilisez la méthode [StandardScaler](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html) de scikit-learn pour normaliser X_train et X_test ainsi que y_train et y_test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/boston_scaler.py\n",
    "from sklearn import preprocessing\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Réseaux de neurones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous allons construire nos réseaux de neurones comme des successions de couches.  \n",
    "Nous distinguerons, similairement aux frameworks les plus courants (Tensorflow, Pytorch, Keras ...) deux types de couches:\n",
    "* des couches de poids et gradients\n",
    "* des couches d'activations \n",
    "\n",
    "Chaque couche possèdera:\n",
    "* une méthode forward qui reçoit en entrée un vecteur $x$ et y applique une transformation qui servira d'inputs pour les couches suivantes\n",
    "* une méthode backward qui reçoit en entrée des gradients et les retropopages vers les couches précédentes. \n",
    "* un argument layer_type permettant de définir le type de couche\n",
    "\n",
    "La classe suivante définit la structure d'une couche:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer:\n",
    "    def __init__(self):\n",
    "        self.layer_type = 'abstract'\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        pass\n",
    "    \n",
    "    def backward(self, grad):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Un réseau de neurones sera donc une classe:\n",
    "* contenant une liste de couches\n",
    "* possédant une méthode forward (qui sera en fait la composition des méthodes forward de ses couches) qui pour un vecteur x fournira une prédiction\n",
    "* et une méthode backward (composition des méthodes forward de ses couches) qui rétropopagera les gradients depuis sa sortie jusqu'a ses premières couches."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![NeuralNetwork](layers.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous allons maintenant implémenter les couches de notre réseau.  \n",
    "Commençons par les couches \"linéaires\" composées de poids $W$ et de biais $b$.  \n",
    "Ces couches reçoivent en entrée un vecteur $x$ et retourne en sortie le résultat de $Wx + b$.\n",
    "\n",
    "Complétez la definition de la classe LinearLayer:\n",
    "* les poids et les biais serons stockés dans un dictionnaire \"params\". Les poids seront une matrice $input \\times output$ initialisée uniformémént entre $[-0.1, 0.1]$ (pensez à la fonction random.uniform de numpy)\n",
    "* implémentez la méthode forward (vous pouvez utiliser la fonction np.matmul pour calculer $Wx$), il faudra garder en mémoire le vecteur d'entrées qui sera utilisé lors de l'appel à la méthode backward:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%load solutions/layers.py\n",
    "class Linear(Layer):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.layer_type = 'linear'\n",
    "        self.params ={}\n",
    "        self.grads = {}\n",
    "        #weights est la matrice [input x output] contenant les poids de la couche \n",
    "        self.params[\"weights\"] = ...\n",
    "        #biais est un vecteur de dimension: (output_size)\n",
    "        self.params[\"bias\"] = ...\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        # outputs = inputs @ weights + biais\n",
    "        #Nous devons garder en mémoire les inputs car ils seront utilisés dans la méthode backward\n",
    "        self.inputs = inputs\n",
    "        return ...\n",
    "    \n",
    "    def backward(self, grad):\n",
    "        # we need to sum gradients over the batch axis\n",
    "        self.grads[\"weights\"] = np.matmul(self.inputs.T, grad)\n",
    "        self.grads[\"bias\"] = np.sum(grad, axis=0)\n",
    "        return np.matmul(grad, self.params[\"weights\"].T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testez votre layer, la céllule suivant ne doit pas renvoyer d'erreur:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size, output_size = 10, 5\n",
    "X = np.random.rand(input_size)\n",
    "grads = np.random.rand(10,5)\n",
    "test_linear_layer = Linear(input_size, output_size)\n",
    "assert test_linear_layer.forward(X).shape == (5,)\n",
    "assert test_linear_layer.backward(grads).shape == (10, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activation Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous allons maintenant implémenter les fonctions d'activation utilisées par les neurones.  \n",
    "Implémentez une fonction d'activation ainsi que sa dérivée (par exemple la fonction ReLU: $f(x) = max(0,x)$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/activations.py\n",
    "def relu(x):\n",
    "    return ...\n",
    "\n",
    "def relu_prime(x):\n",
    "    return ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implémentez maintenant les couches d'activations:  \n",
    "Elles seront initialisées à partir d'une fonction $f$ et de sa dérivée"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%load  solutions/activation_layer.py\n",
    "class Activation(Layer):\n",
    "    def __init__(self, f, f_prime):\n",
    "        super().__init__()\n",
    "        self.layer_type = 'activation'\n",
    "        self.f = f\n",
    "        self.f_prime = f_prime\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        #Nous devons garder en mémoire les inputs car ils seront utilisés dans la méthode backward\n",
    "        self.intputs = inputs\n",
    "        return ...\n",
    "    \n",
    "    def backward(self, grad):\n",
    "        \"\"\" si y = f(x) et x = g(z)\n",
    "        alors dy/dz = f'(x) * g'(z)\n",
    "        Dans notre cas g'(z) correspond aux gradients en entrée\n",
    "        \"\"\"\n",
    "        return self.f_prime(self.intputs) * grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testez votre couche d'activation, la cellule suivante ne dois pas renvoyer d'erreur:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relu_layer = Activation(f=relu, f_prime=relu_prime)\n",
    "\n",
    "#forward\n",
    "x = np.array([-1, 5, -2, 6])\n",
    "y = relu_layer.forward(x)\n",
    "assert np.array_equal(y, np.array([0, 5, 0, 6]))\n",
    "\n",
    "#backward\n",
    "incomming_grads = np.array([-0.1, 0.6, -0.4, -0.1])\n",
    "grads = relu_layer.backward(incomming_grads)\n",
    "assert np.array_equal(grads, np.array([-0. ,  0.6, -0. , -0.1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous allons maintenant définir la classe qui sera utilisée pour représenter nos réseaux de neurones.  \n",
    "Un réseau de neurones possédera une liste de couches et deux méthodes: forward et backward.  \n",
    "\n",
    "Complétez la méthode forward et la méthode backward de la classe NeuralNetwork en appellant les methodes forward et backward des couches du réseau:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/neural_network.py\n",
    "class NeuralNetwork:\n",
    "    def __init__(self, layers):\n",
    "        self.layers = layers\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        ...\n",
    "\n",
    "    def backward(self, grad):\n",
    "        ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous avons maintenant tous les éléments constituant un réseau de neurones.  \n",
    "Instanciez un reseau de neurones constitué de 3 couches:\n",
    "* une couche d'entrée recevant un vecteur de dimension 13 et constituée de 40 neurones\n",
    "* une couche cachée de 40 neurones\n",
    "* une couche de sortie constituée d'un seul neurone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/boston_network.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testez votre réseau sur le jeu de test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "y_pred = net.forward(X_test)\n",
    "print(mean_absolute_error(y_pred, y_test))\n",
    "    \n",
    "plt.figure(figsize=(20,5))\n",
    "plt.scatter(range(len(y_test)), scaler.inverse_transform(y_test), label='target')\n",
    "plt.scatter(range(len(y_test)), scaler.inverse_transform(y_pred), label='prediction')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les prédictions du réseau sont mauvaise.  \n",
    "C'est normal on ne l'a pas encore entrainé.  \n",
    "Commençons par implémenter la fonction de perte qu'utilisera le réseau de neurones dans son apprentissage.  \n",
    "Nous sommes ici dans un problème de régression, nous utiliserons l'erreur quadratique moyenne (Mean Squared Error) pour apprendre notre modèle:\n",
    "$$\\sum_{i}^{n}\\frac{(f(x_i) - y_i)^2}{n}$$  \n",
    "Implémentez la fonction de perte ainsi que sa dérivée qui sera utilisée lors de la retro-propagation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/mse.py\n",
    "class MSE():\n",
    "\n",
    "    def loss(self, y_pred, y_true):\n",
    "        #returns a scalar\n",
    "        return ...\n",
    "    \n",
    "    def grad(self, y_pred, y_true):\n",
    "        #returns a tensor of gradients\n",
    "        return ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous allons maintenant impémenter la méthode d'optimisation utilisée durant l'apprentissage.\n",
    "Nous utiliserons ici la descente de gradient stochastique.\n",
    "La méthode est simple à chaque pas nous allons mettre à jour les paramètres du réseau grace à la formule suivante: $w_{t+1} = w_t - \\eta \\nabla$ où $\\eta$ est le learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/sgd.py\n",
    "class SGD():\n",
    "    def __init__(self, lr=0.001):\n",
    "        self.lr = lr\n",
    "    \n",
    "    def step(self, net):\n",
    "        for layer in net.layers:\n",
    "            if layer.layer_type == 'linear':\n",
    "                for param, grad in zip(layer.params.values(), layer.grads.values()):\n",
    "                    ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fit function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il ne reste plus qu'à implémenter la fonction fit qui réalise une époque d'apprentissage.  \n",
    "La fonction doit:\n",
    "* calculer les predictions du réseau sur le batch d'apprentissage\n",
    "* calculer la loss\n",
    "* calculer le gradient de la loss\n",
    "* retropropager les gradients\n",
    "* realiser un pas d'optimisation\n",
    "* retourner la perte "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/fit.py\n",
    "def fit(net, loss, optimizer, X, y):\n",
    "    ...\n",
    "    return prediction_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On peut maintant entrainer notre réseau:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = SGD(lr=0.01)\n",
    "loss = MSE()\n",
    "for i, (x, y) in enumerate(zip(X_train, y_train)):\n",
    "    training_loss = fit(net, loss, optimizer, np.array([x]), np.array([y]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Affichez les résultats de votre entrainement:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = net.forward(X_test)\n",
    "print(mean_absolute_error(y_pred, y_test))\n",
    "    \n",
    "plt.figure(figsize=(20,5))\n",
    "plt.scatter(range(len(y_test)), scaler.inverse_transform(y_test), label='target')\n",
    "plt.scatter(range(len(y_test)), scaler.inverse_transform(y_pred), label='prediction')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Digits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Essayons maintenant notre classe sur un problème de classification.  \n",
    "Nous allons chercher à classifier des chiffres manuscrits à partir des pixels. Nous utiliserons pour cela le dataset [digits de scikit-learn](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_digits.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_digits\n",
    "\n",
    "dataset = load_digits()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Affichez les 10 premiers éléments du dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load  solutions/digits.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similairement au dataset précédent séparer votre jeu de donné en deux et normalisez les vecteurs d'entrées. (Pas la peine cette fois çi de normaliser les y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/digits_preprocessing.py\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "data = np.asarray(dataset.data, dtype='float32')\n",
    "target = np.asarray(dataset.target, dtype='int32')\n",
    "\n",
    "X_train, X_test, y_train, y_test = ...\n",
    "\n",
    "...\n",
    "X_train = ...\n",
    "X_test = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous sommes cette fois-ci en présence d'un problème de classification.  \n",
    "La fonction de perte MSE n'est plus apprpriée pour notre problème.  \n",
    "Nous allons cette fois çi-utiliser la [cross-entropy.](https://en.wikipedia.org/wiki/Cross_entropy#Cross-entropy_loss_function_and_logistic_regression) comme fonction de perte:\n",
    "$$ L = \\frac{1}{N}\\sum_{i=1}^{N}y_i \\cdot log(\\hat{y_i})$$  \n",
    "Avec $\\hat{y_i}$ la prédiction du réseau sur laquelle est appliqué la fonction softmax:\n",
    "$$softmax_j(x) = \\frac{e^x_j}{\\sum_{k}e^x_k}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    return np.exp(x) / np.exp(x).sum(axis=-1,keepdims=True)\n",
    "\n",
    "class CrossEntropy():\n",
    "\n",
    "    def loss(self, logits, y_true):\n",
    "        logits_for_answers = logits[np.arange(len(logits)), y_true]\n",
    "        xentropy = - logits_for_answers + np.log(np.sum(np.exp(logits),axis=-1))\n",
    "        return xentropy\n",
    "\n",
    "\n",
    "\n",
    "    def grad(self, logits, y_true):\n",
    "        ones_for_answers = np.zeros_like(logits)\n",
    "        ones_for_answers[np.arange(len(logits)), y_true] = 1\n",
    "        y_softmax = softmax(logits)\n",
    "        return (- ones_for_answers + y_softmax) / logits.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instanciez un reseau de neurones capable de prédire la classe d'une image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%load solutions/digits_net.py\n",
    "...\n",
    "net = ...\n",
    "optimizer = SGD(lr=0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculer la précision de votre réseau sur le jeu de test. (Vous pouvez pour cela utiliser la fonction [acuracy_score](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html) de scikit-learn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%load  solutions/accuracy1.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La fonction suivante permet de visualiser nos prédiction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_prediction(net, sample_idx=range(3), classes=range(10)):\n",
    "    \n",
    "    for idx in sample_idx:\n",
    "        plt.figure()\n",
    "        logits = net.forward(X_test[idx])\n",
    "        probas = softmax(logits)\n",
    "        prediction = np.argmax(probas)\n",
    "\n",
    "        fig, (ax0, ax1) = plt.subplots(nrows=1, ncols=2, figsize=(10, 4))\n",
    "\n",
    "        ax0.imshow(scaler.inverse_transform(X_test[idx]).reshape(8, 8), cmap=plt.cm.gray_r,\n",
    "                   interpolation='nearest')\n",
    "        ax0.set_title(\"True image label: %d\" % y_test[idx]);\n",
    "        ax1.bar(classes, np.eye(len(classes))[y_test[idx]], label='true')\n",
    "        ax1.bar(classes, probas, label='prediction', color=\"red\")\n",
    "        ax1.set_xticks(classes)\n",
    "\n",
    "        ax1.set_title('Output probabilities (prediction: %d)'\n",
    "                      % prediction)\n",
    "        ax1.set_xlabel('Digit class')\n",
    "        ax1.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_prediction(net, sample_idx=range(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous allons cette fois-çi, utiliser des mini-batchs durant notre apprentissage.  \n",
    "La fonction suivante permet d' itérer sur des mini-batchs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iterate_minibatches(inputs, targets, batchsize, shuffle=True):\n",
    "    assert inputs.shape[0] == targets.shape[0]\n",
    "    if shuffle:\n",
    "        indices = np.arange(inputs.shape[0])\n",
    "        np.random.shuffle(indices)\n",
    "    for start_idx in range(0, inputs.shape[0] - batchsize + 1, batchsize):\n",
    "        if shuffle:\n",
    "            excerpt = indices[start_idx:start_idx + batchsize]\n",
    "        else:\n",
    "            excerpt = slice(start_idx, start_idx + batchsize)\n",
    "        yield inputs[excerpt], targets[excerpt]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Et la fonction suivant de réaliser une époque d'apprentissage à l'aide de mini-batchs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_one_epoch(X, y_true):\n",
    "    for x, y in iterate_minibatches(X, y_true, 32):\n",
    "        training_loss = fit(net, loss, optimizer, x, y)\n",
    "        \n",
    "    return training_loss.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entrainez votre reseau sur 15 époques.  \n",
    "A chaque époque calculez votre précision sur le jeu de train et sur le jeu de test et affichez les sur un même graphique une fois l'entrainement terminé."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%load solutions/plot_accuracy.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Affichez votre précision sur le jeu de test et visualisez vos prédiction à l'aide de la fonction plot_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/accuracy2.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Essayez d'afficher votre matrice de confusion en vous inspirant de [ce code:](https://scikit-learn.org/dev/auto_examples/classification/plot_digits_classification.html#sphx-glr-auto-examples-classification-plot-digits-classification-py) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/confusion_matrix.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Affichez les exemples sur lesquels le réseau se trompe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/plot_errors.py"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
